{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract_data_eicu\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from common_eicu import *\n",
    "from math import isnan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPACT_MODE = False\n",
    "COMPACT_MODE = True\n",
    "\n",
    "TEST_MODE = False\n",
    "# TEST_MODE = True\n",
    "\n",
    "# COMPRESS_OUTPUT = False\n",
    "COMPRESS_OUTPUT = True\n",
    "\n",
    "FILTERED_LAB_PATH = relative_path('lab_filtered.csv.gz')\n",
    "FILTERED_EXAM_PATH = relative_path('exam_filtered.csv.gz')\n",
    "FILTERED_TREATMENT_PATH = relative_path('treatment_filtered.csv.gz')\n",
    "\n",
    "OUTPUT_PATH = relative_path(\n",
    "    'data_eicu_'\n",
    "    + ('compact' if COMPACT_MODE else 'full')\n",
    "    + '_raw'\n",
    "    + ('_test' if TEST_MODE else '')\n",
    "    + '.csv'\n",
    "    + ('.gz' if COMPRESS_OUTPUT else '')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity -> { col -> { 'offsets': [], 'values': []  } }\n",
    "temporal_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_compact(df):\n",
    "\n",
    "    def map_age(age):\n",
    "        if age != age:\n",
    "            return age\n",
    "        elif age == '> 89':\n",
    "            return 90\n",
    "        else:\n",
    "            return int(age)\n",
    "\n",
    "    df['age'] = df['age'].map(map_age)\n",
    "\n",
    "    df['vasopressor'] = df['vasopressor'].map(\n",
    "        lambda v: v if v != v else int(v)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_full(df):\n",
    "\n",
    "    post_process_compact(df)\n",
    "\n",
    "    df['bmi'] = df['weight'] / (df['height'] / 100) ** 2\n",
    "    del df['weight']\n",
    "    del df['height']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the map of non-temporal data sources\n",
    "data_sources = {}\n",
    "with open(CATALOGUE_PATH, 'r') as catalogue_file:\n",
    "    catalogue = json.load(catalogue_file)\n",
    "    for column_name in REQUIRED_COLUMNS:\n",
    "        if not column_name in catalogue:\n",
    "            raise Exception(\n",
    "                f'Cannot find column \"{column_name}\" in catalogue!'\n",
    "            )\n",
    "        file_path = catalogue[column_name]\n",
    "        if file_path in data_sources:\n",
    "            data_sources[file_path].append(column_name)\n",
    "        else:\n",
    "            data_sources[file_path] = [column_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect non-temporal data frames\n",
    "data_frames = []\n",
    "for input_path, column_names in data_sources.items():\n",
    "    usecols = [KEY_IDENTITY] + column_names\n",
    "    data_frame = pd.read_csv(\n",
    "        input_path,\n",
    "        usecols=usecols,\n",
    "        index_col=KEY_IDENTITY,\n",
    "    )\n",
    "    data_frame.columns = map(\n",
    "        map_column_name,\n",
    "        data_frame.columns,\n",
    "    )\n",
    "    data_frames.append(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 148502 entries, 141168 to 3353263\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   age                148502 non-null  object \n",
      " 1   urine              148502 non-null  float64\n",
      " 2   wbc                148502 non-null  float64\n",
      " 3   meanbp             148502 non-null  float64\n",
      " 4   actualhospitallos  148502 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# join non-temporal data frames\n",
    "df_non_temporal = reduce(\n",
    "    lambda df_0, df_1: df_0.join(df_1),\n",
    "    data_frames,\n",
    ")\n",
    "df_non_temporal.drop_duplicates(inplace=True)\n",
    "df_non_temporal.dropna(inplace=True)\n",
    "df_non_temporal.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init indices of temporal data\n",
    "for index in df_non_temporal.index:\n",
    "    temporal_data[index] = {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n"
     ]
    }
   ],
   "source": [
    "df_treatment = pd.read_csv(\n",
    "    FILTERED_TREATMENT_PATH,\n",
    "    usecols=TREATMENT_USE_COLS,\n",
    "    nrows=(TEST_ROWS if TEST_MODE else None),\n",
    ")\n",
    "\n",
    "# init treatment columns in temporal_data\n",
    "for record in temporal_data.values():\n",
    "    for keyword in REQUIRED_TREATMENT_KEYWORDS:\n",
    "        column_name = map_column_name(keyword)\n",
    "        record[column_name] = {\n",
    "            'offsets': [MIN_OFFSET],\n",
    "            'values': [False],\n",
    "        }\n",
    "\n",
    "# collect treatment info\n",
    "treatment_iterator = SimpleProgress(df_treatment.index)\n",
    "for index in treatment_iterator:\n",
    "    identity = df_treatment.at[index, KEY_IDENTITY]\n",
    "    if not identity in temporal_data:\n",
    "        continue\n",
    "    raw_value = df_treatment.at[index, KEY_TREATMENT_STRING]\n",
    "    treatment_string = str(raw_value).lower()\n",
    "    for keyword in REQUIRED_TREATMENT_KEYWORDS:\n",
    "        if not keyword in treatment_string:\n",
    "            continue\n",
    "        offset = df_treatment.at[index, KEY_TREATMENT_OFFSET]\n",
    "        column_name = map_column_name(keyword)\n",
    "        store = temporal_data[identity][column_name]\n",
    "        store['offsets'].append(offset)\n",
    "        store['values'].append(True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exam = pd.read_csv(\n",
    "    FILTERED_EXAM_PATH,\n",
    "    usecols=EXAM_USE_COLS,\n",
    "    nrows=(TEST_ROWS if TEST_MODE else None),\n",
    ")\n",
    "\n",
    "# init exam columns in temporal_data\n",
    "for record in temporal_data.values():\n",
    "    for name in REQUIRED_EXAM_ITEMS:\n",
    "        column_name = map_column_name(name)\n",
    "        record[column_name] = {\n",
    "            'offsets': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "# collect exam items\n",
    "exam_iterator = SimpleProgress(df_exam.index)\n",
    "for index in exam_iterator:\n",
    "    identity = df_exam.at[index, KEY_IDENTITY]\n",
    "    if not identity in temporal_data:\n",
    "        continue\n",
    "    item_name = df_exam.at[index, KEY_EXAM_NAME]\n",
    "    if not item_name in REQUIRED_EXAM_ITEMS:\n",
    "        continue\n",
    "    column_name = map_column_name(item_name)\n",
    "    offset = df_exam.at[index, KEY_EXAM_OFFSET]\n",
    "    value = df_exam.at[index, KEY_EXAM_RESULT]\n",
    "    store = temporal_data[identity][column_name]\n",
    "    store['offsets'].append(offset)\n",
    "    store['values'].append(value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab = pd.read_csv(\n",
    "    FILTERED_LAB_PATH,\n",
    "    usecols=LAB_USE_COLS,\n",
    "    nrows=(TEST_ROWS if TEST_MODE else None),\n",
    ")\n",
    "\n",
    "# init lab columns in temporal_data\n",
    "for record in temporal_data.values():\n",
    "    for name in REQUIRED_LAB_VARIABLES:\n",
    "        column_name = map_column_name(name)\n",
    "        record[column_name] = {\n",
    "            'offsets': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "# collect lab variables\n",
    "lab_iterator = SimpleProgress(df_lab.index)\n",
    "for index in lab_iterator:\n",
    "    identity = df_lab.at[index, KEY_IDENTITY]\n",
    "    if not identity in temporal_data:\n",
    "        continue\n",
    "    var_name = df_lab.at[index, KEY_LAB_NAME]\n",
    "    if not var_name in REQUIRED_LAB_VARIABLES:\n",
    "        continue\n",
    "    var_record = lab_var_dict[var_name]\n",
    "    column_name = map_column_name(var_name)\n",
    "    offset = df_lab.at[index, KEY_LAB_OFFSET]\n",
    "    value = df_lab.at[index, KEY_LAB_RESULT]\n",
    "    store = temporal_data[identity][column_name]\n",
    "    store['offsets'].append(offset)\n",
    "    store['values'].append(value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct temporal data rows\n",
    "\n",
    "temporal_column_names = list(\n",
    "    map(map_column_name, [\n",
    "        KEY_IDENTITY,\n",
    "        KEY_OFFSET,\n",
    "        *REQUIRED_TREATMENT_KEYWORDS,\n",
    "        *REQUIRED_EXAM_ITEMS,\n",
    "        *REQUIRED_LAB_VARIABLES,\n",
    "    ])\n",
    ")\n",
    "temporal_data_rows = []\n",
    "\n",
    "temporal_data_iterator = SimpleProgress(temporal_data.items())\n",
    "for identity, record in temporal_data_iterator:\n",
    "\n",
    "    begin = max(min(store['offsets']) for store in record)\n",
    "    end = max(max(store['offsets']) for store in record)\n",
    "    if begin < MIN_OFFSET:\n",
    "        begin = MIN_OFFSET\n",
    "\n",
    "    for offset in range(begin, end + 1):\n",
    "        row = []\n",
    "\n",
    "        for column_name in temporal_column_names:\n",
    "\n",
    "            if column_name == KEY_IDENTITY:\n",
    "                row.append(identity)\n",
    "                continue\n",
    "            elif column_name == KEY_OFFSET:\n",
    "                row.append(offset)\n",
    "                continue\n",
    "\n",
    "            store = record[column_name]\n",
    "            offsets = store['offsets']\n",
    "\n",
    "            try:\n",
    "                index = offsets.index(offset)\n",
    "            except ValueError:  # not found\n",
    "                row.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            value = store['values'][index]\n",
    "            row.append(value)\n",
    "\n",
    "        temporal_data_rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal = pd.DataFrame(\n",
    "    temporal_data_rows,\n",
    "    columns=temporal_column_names,\n",
    ")\n",
    "df_temporal.groupby(KEY_IDENTITY).ffill(inplace=True)\n",
    "df_temporal.dropna(inplace=True)\n",
    "df_temporal.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join temporal and non-temporal data frames\n",
    "df_output = pd.merge(\n",
    "    df_non_temporal,\n",
    "    df_temporal,\n",
    "    on=KEY_IDENTITY,\n",
    ")\n",
    "df_output.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPACT_MODE:\n",
    "    post_process_compact(df_output)\n",
    "else:\n",
    "    post_process_full(df_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.dropna(inplace=True)\n",
    "\n",
    "if COMPRESS_OUTPUT:\n",
    "    df_output.to_csv(\n",
    "        OUTPUT_PATH,\n",
    "        compression='gzip',\n",
    "    )\n",
    "else:\n",
    "    df_output.to_csv(\n",
    "        OUTPUT_PATH,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7976576504ac6c456dadd405d7477574ca2a64265ee4724cfbc25daae5f6d94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
